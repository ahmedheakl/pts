# Model identifiers can be HF model ids or internal checkpoints.
diffusion:
  model_id: "GSAI-ML/LLaDA-8B-Instruct" #"Dream-org/Dream-v0-Instruct-7B" 
  device: "cuda:0"      # or "cpu"
  max_new_tokens: 256

llm:
  model_id: "meta-llama/Llama-3.1-8B-Instruct"
  device: "cuda:1"
  max_new_tokens: 512
  temperature: 0.2
  do_sample: false

llm_gpt:
  model_id: "gpt-4o"
  max_new_tokens: 512
  temperature:  0.2
  do_sample: false

runtime:
  output_dir: "outputs"
  json_prefix: "run"
  seed: 42

prompting:
  # Template for the *final* LLM call that takes concatenated input
  final_system_msg: ""
  concat_header_plan: "## PLAN"
  concat_header_refined: "## REFINED PLAN"
  concat_header_extra: "## LLM Instructions"
